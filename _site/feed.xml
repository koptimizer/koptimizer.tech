<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.2">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2022-03-18T18:00:18+09:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Koptimizer.tech</title><subtitle>Exploration &amp; Exploitation</subtitle><author><name>Koptimizer</name></author><entry><title type="html">Paper Review - Can Wikipedia Help Offline Reinforcement Learning?</title><link href="http://localhost:4000/wikipedea.html" rel="alternate" type="text/html" title="Paper Review - Can Wikipedia Help Offline Reinforcement Learning?" /><published>2022-03-18T00:00:00+09:00</published><updated>2022-03-18T00:00:00+09:00</updated><id>http://localhost:4000/wikipedea</id><content type="html" xml:base="http://localhost:4000/wikipedea.html"><![CDATA[<p>22년 3월 기준으로 최신 RL 논문 중 가장 충격적인 논문을 한 편 뽑으라고 하면 단언코 이 논문을 고를 수 있을 것이다.</p>

<p>can wikipedia help offline reinforcement learning?</p>

<p>abstract</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Fine-tuning reinforcement learning (RL) models has been challenging because of a lack of large scale off-the-shelf datasets as well as high variance in transferability among different environments. Recent work has looked at tackling offline RL from the perspective of sequence modeling with improved results as result of the introduction of the Transformer architecture. However, when the model is trained from scratch, it suffers from slow convergence speeds. In this paper, we look to take advantage of this formulation of reinforcement learning as sequence modeling and investigate the transferability of pre-trained sequence models on other domains (vision, language) when finetuned on offline RL tasks (control, games). To this end, we also propose techniques to improve transfer between these domains. Results show consistent performance gains in terms of both convergence speed and reward on a variety of environments, accelerating training by 3-6x and achieving state-of-the-art performance in a variety of tasks using Wikipedia-pretrained and GPT2 language models. We hope that this work not only brings light to the potentials of leveraging generic sequence modeling techniques and pre-trained models for RL, but also inspires future work on sharing knowledge between generative modeling tasks of completely different domains.
</code></pre></div></div>

<p>작성중</p>]]></content><author><name>Koptimizer</name></author><category term="opinion" /><category term="Reinforcement Learning" /><category term="Machine Learning" /><category term="Paper Review" /><summary type="html"><![CDATA[Fine-tuning reinforcement learning (RL) models has been challenging because of a lack of large scale off-the-shelf datasets as well as high variance in transferability among different environments.]]></summary></entry></feed>